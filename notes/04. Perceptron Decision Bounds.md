#geometric #decision-bounds #margin #perceptron #normalize #normalization #single-layer

## 0. Algorithm
---
##### PerceptronTrain(D, $MaxIter$)
---
$w_d \leftarrow 0$ for all $d=1 \dots D$               // init. weights
$b \leftarrow 0$                                                   // init. bias
**for** $iter = 1\dots MaxIter$ **do**:
	**for** all $(x, y) \in D$ **do**:
		$a \leftarrow \sum_{d=1}^D w_dx_d+b$                   // compute activation for this example
		**if** $ya \leq 0$ **then**:
			$w_d \leftarrow w_d+yx_d$, for all $d=1\dots D$            // update weights
			$b \leftarrow b + y$                                                         // update bias
**return** $w_0, w_1,\dots,w_D, b$

---
**NOTE**: $(x, y) \in D$, where $x$: the $i^{th}$ feature of the input vector and $y$: the desired ouput of that feature.


---
##### PerceptronTrain(D, $MaxIter$)
---
$a \leftarrow \sum_{d=1}^D w_d\hat{x}_d+b$                            // compute activation for test example
**return** $SIGN(a)$

---


## 1. Error-driven update
---
- is *online*, i.e. only looks at one problem at any given time.
- is *error-driven*, i.e. does not update as long as it perform well.

> [!note]
> The new activation *is always* **at least** *the old activation plus one*, because the quantity $(\sum_dx_d^2 \Rightarrow x_d^2) \geq 0$.


$$
\begin{aligned}
a' &= \sum_{d=1}^D w'_dx_d+b' \cr
 &= \sum_{d=1}^D (w_d+x_d)x_d + (b+1) \cr
 &= \sum_{d=1}^D w_dx_d + b + \sum_{d=1}^{D}x_dx_d+1 \cr
 &= a + \sum_{d=1}^D x_d^2 + 1 \quad > \quad a
\end{aligned}
$$


## 2. Geometric Interpretation
---
There is still the need for *geometric interpretation*, i.e. the same problem encountered by the KNN (supervised) and clustering methods (unsupervised).

Decision boundary for perceptron is a plane.  
$$
	\mathcal{B} = \left\{x: \sum_dw_dx_d =0 \right\}
$$

Recall that $\sum_d w_dx_d$ is the *dot product* $\boldsymbol{\vec{w}\cdot\vec{x}}$, where $\boldsymbol{\vec{w}} = \langle w_1, w_2, \dots, w_D\rangle$. This means that the decision boundary is the plane perpendicular to $\boldsymbol{w}$.

**NOTE:** it is common to #normalize weight vectors, because we only consider their **sign** which makes their *scale* irrelevant. For example, $2w$ has the same meaning as $w$ to the classifier.


## 3. Interpreting perceptron weights
---
How *sensitive* is the perceptron to **small changes** in some particular feature?

We determine this by taking the derivative of, let's say some $i^{th}$ feature/column:
$$
	\frac{\partial}{\partial x_i} (\sum_d w_dx_d + b) = w_i
$$
Thus, the rate of change of feature $i$'s activation is exactly $w_i$. From observation, we typically *sort all the weights from in descending order (positive to negative), and then take the top 10 and bottom 10*. These positive cluster is most sensitive to positive prediction, and vice versa.


## 4. Convergence & Linear Separability
---
The perceptron converges whenever it is even remotely possible to converge. Now, how long does it take to converge? The perceptron converges faster for *easy* over *hard* problems. We determine the hardness of the problem via **margin**.

Given a data set **D**, a weight vector $w$ and bias $b$, the margin of $w$, $b$ on **D** is defined as:
$$
	margin(\mbox{\textbf{D}}, w, b) = \cases {
		min_{(x, y)\in D} (\boldsymbol{w \cdot x} + b) & if $\boldsymbol{w}$ separates \textbf{D}. \cr
		-\infty      & otherwise.
	}
$$
Following that, the *margin of the data set* is the largest achievable margin on this data:
$$
	\DeclareMathOperator*{\sup}{sup}
	margin(\mbox{\textbf{D}}) = \boldsymbol\max_{w, b} margin(\mbox{\textbf{D}}, w, b) 
$$

## 5. Limits & Practical Considerations
---
Its decision boundaries can only be *linear*. The **XOR problem** shows this inherent limit.

|     | 0   | 1   | 
| --- | --- | --- |
| 0   | +   | -   |
| 1   | -   | +   |


This distribution is impossible to be partitioned with a linear boundary.