#geometric #decision-bounds #margin


**NOTE:** *Add a description of the algorithm for future reference.*

### 1. Perceptron follows the *error-driven updating* model
- is *online*, i.e. only looks at one problem at any given time.
- is *error-driven*, i.e. does not update as long as it perform well.

> [!note]
> The new activation *is always* **at least** *the old activation plus one*, because the quantity $(\sum_dx_d^2 \Rightarrow x_d^2) \geq 0$.

$$
\begin{aligned}
a' &= \sum_{d=1}^D w'_dx_d+b' \cr
 &= \sum_{d=1}^D (w_d+x_d)x_d + (b+1) \cr
 &= \sum_{d=1}^D w_dx_d + b + \sum_{d=1}^{D}x_dx_d+1 \cr
 &= a + \sum_{d=1}^D x_d^2 + 1 \quad > \quad a
\end{aligned}
$$


2. There is still the need for *geometric interpretation*, i.e. the same problem encountered by the KNN (supervised) and clustering methods (unsupervised).


3. Interpreting perceptron *weights*.

## 4. *Convergence* and *Linear Separability*

The perceptron converges whenever it is even remotely possible to converge. Now, how long does it take to converge? The perceptron converges faster for *easy* over *hard* problems. We determine the hardness of the problem via **margin**.


## 5. Limits and Practical Considerations

Its decision boundaries can only be *linear*. The **XOR problem** shows this inherent limit.

|     | 0   | 1   | 
| --- | --- | --- |
| 0   | +   | -   |
| 1   | -   | +   |

This distribution is impossible to be partitioned with a linear boundary.