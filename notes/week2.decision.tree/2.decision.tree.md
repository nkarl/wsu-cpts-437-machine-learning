# Decision Trees

- *nominal features* (as opposed to continuous features), i.e. they are labels instead of values.

Given a decision tree, each path from root to leaf is a conjunction of all the attributes along the path.  Furthermore, this means that the tree is the disjunction of all the possible paths.

A greedy top-down decision is less computationally expensive.

Decision tree is inspired information theory.  Thus, entropy applies and is a measure of the degree of noise/impurity of the data set.

$$Entropy(S) = -p_{+}log_2p_{+} - p_{-}log_2p_{-}$$

$$Gain(S, A) = Entropy(S) - \sum_{i} \frac{|S_v|}{S} Entropy(S_v)$$
