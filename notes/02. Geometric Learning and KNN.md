#geometric #dimensionality #nearest-neigbors #KNN #decision-bounds #normalize #normalization 

## 1. Geometry & Dimensionality
---
Given a set of email data, we see if we can identify spam emails from regular emails. We aim to build feature vectors and let the algorithm to "learn". There are a few suggestions we can start from:
- subject length
- domain name of email
- punctuation
- grammar score
- capitalization
- words (targeted words)
- formatting
- no specific person being addressed

Thus, a good way to break down the problem is to use features as *dimensions*.


## 2. K-Nearest Neighbors (KNN) Classifier
---
In a KNN algorithm, we give *data points that are closer to one another* the same label. In other words, data instances should be similar to the points nearby them. We can apply the Euclidean distance to map out the groups of datapoints that closer to those similar to them. The partitioning (drawing the boundaries) task follows after that.

$$d(a,b) = \left( \sum_{d=1}^{D} (a_d - b_d)^2 \right) ^{\frac{1}{2}}$$

In this algorithm, we define $K$ as the number of points nearest to the one that needs labeling. We then take a sum of the K nearest neighbors and depending on the *sign* of the sum, we will label the current point accordingly.

**NOTE:** *Add a description of the algorithm here for future reference.*

## 3. Decision Boundaries
---



## 4. When can a distance function cause the classifer to fail?
---
1. Irrelevant features
2. Redudant features
3. Range of features

It is worth noting that the KNN classifier is prone to *overfitting* label noise. A prime example for this is when we need to label a datapoint is 'plus', but because it is in the middle of a group of 'minus', it ends up being labeled 'minus'. This is because the algorithm has no way to know; it does exactly what it is designed to do: labeling a datapoint according to its nearest neighbors.

A way to counter this problem is to [[05. Practical Considerations|normalize data]].

```python

def normalize(X):
	min_v = min(X)
	range = float(max(X) - min_v)
	new_list = [(x-min_v)/range for x in X]
	
```


## 4. KNN Pros & Cons
---
#### Pros:
- simple
- no training
- good accuracy
- time series
- classfication or regression

#### Cons:
- expensive; does NOT scale well
- sensitive to features & distance
