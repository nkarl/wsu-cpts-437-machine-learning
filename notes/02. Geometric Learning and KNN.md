#geometric #dimensionality #nearest-neigbors #KNN #decision-bounds #normalize #normalization 

## 1. Geometry & Dimensionality
---
Given a set of email data, we see if we can identify spam emails from regular emails. We aim to build feature vectors and let the algorithm to "learn". There are a few suggestions we can start from:
- subject length
- domain name of email
- punctuation
- grammar score
- capitalization
- words (targeted words)
- formatting
- no specific person being addressed

Thus, a good way to break down the problem is to use features as *dimensions*.


## 2. K-Nearest Neighbors (KNN) Classifier
---
In a KNN algorithm, we give *data points that are closer to one another* the same label. In other words, data instances should be similar to the points nearby them.

$$d(a,b) = \left( \sum_{d=1}^{D} (a_d - b_d)^2 \right) ^{\frac{1}{2}}$$


## 3. When can a distance function cause the classifer to fail?
---
1. Irrelevant features
2. Redudant features
3. Range of features

A way to counter this problem is to [[05. Practical Considerations|normalize data]].

```python

def normalize(X):
	min_v = min(X)
	range = float(max(X) - min_v)
	new_list = [(x-min_v)/range for x in X]
	
```


## 4. KNN Pros & Cons
---
#### Pros:
- simple
- no training
- good accuracy
- time series
- classfication or regression

#### Cons:
- expensive; does NOT scale well
- sensitive to features & distance
