#autonomy #adaptability #efficiency #supervise #unsupervise #inductive-learning 

[[00. ML Overview#3. The Induction Framework#Types of Inductive Learning|types of inductive learning]], [[00. ML Overview#5. Prediction Accuracy|prediction accuracy]], [[00. ML Overview#6. Performance: Loss Function|loss function]] 

## 1. What is Machine Learning?
---
- A computer program that improves its performance at some task through experience.


## 2. Subfields of ML
---
1. Supervised Learning
	- Tree:
		- Decision Tree
		- Random forest
		- Regression Tree, Decision Stump, ...
	- Instance:
		- KNN
	- Kernel:
		- SVM
	- Network:
		- Perceptron
		- Linear Regression
		- Multi-layer
	- Probabilistic Model:
		- Naive Bayes
		- Logistic Regression
		- Bayes Net, Deep Belief Net, Bayesian Optimal, ...

2. Unsupervised Learning
	- Clustering:
		- K-Means
		- DBScan
		- Hierarchical, Gaussian Mixture Models, ...
	- Dimensionality Reduction:
		- PCA
		- SVD, LDA, ...
	- Anomaly Detection
	- Association Rule

3. Reinforcement Learning
	- Temporal Difference
	- Q Learning
	- MDP
	- SARSA
	- Deep Reinforcement Learning

How do we *supervise* the algorithm?  What does it mean to *supervise*?  Supervision here means that *we provide a set of examples, and then the algorithm will use this set to produce* **predictions**.

First, we need to describe the *features*/attributes of the data set, i.e. *data representation*.  In the case of CSV format, the *feature values* are represented in comma-separated values.

**Note:** the concept of *noise* will be discussed more in the future.  In brief, noise could appear as inaccurate labeling of data, for example.


## 3. The Induction Framework
---

```mermaid
graph LR
	1[train data] --> 2[learning algo.] --> 3[f]
	3 --predicted label--> 4[test example] --> 3
    
```

$f(x) \to y$
    where
	    $x$: examples
	    $y$: label

#### Types of Inductive Learning
1. Regression: falls back toward the *mean*
2. Binary classification: predicts a simple binary outcome
3. Multiclass classification: beyond binary outcomes to higher dimensional outcomes
4. Discovery
5. Reinforced learning

```ad-note
 *We focus on 1-4 for now.*
```

#### Example
$f(x) \to R^+$
$f(x) \to {0, 1} {+, -}$
$f(x) \to { NYC, SF, Pullman, CA }$


## 4. Prediction Accuracy
---
$$Accuracy  = \frac {\mbox{amount of correct}}{\mbox{amount of total}}$$


## 5. Performance: Loss Function
---

$L(y, \hat{y})$
    where
        $y$  = true label
        $\hat{y}$ = 

Used in:
- Regression
- Binary classif.
- Multiclass classsif.
- Discovery
- Reinforcement learning


## 6. Other key ideas
---
- variance and bias
- Sigmoid function
- probabilistic modeling
- Naive Bayes