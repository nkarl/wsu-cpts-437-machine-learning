#autonomy #adaptability #efficiency #supervise #unsupervise #reinforcement #inductive-learning #loss-function #probabilistic-model #inductive-bias

[[00. ML Overview#^3e5928|common learning algorithms]], [[00. ML Overview#3. The Induction Framework#Types of Inductive Learning|types of inductive learning]], [[00. ML Overview#4. Prediction Accuracy|prediction accuracy]], [[00. ML Overview#5. Performance: Loss Function|loss function]], [[00. ML Overview#6. Inductive Bias|inductive bias]]

## 1. What is Machine Learning?
---
Machine learning is a process in which a computer program/algorithm improves its performance at some task through experience.


## 2. Subfields of ML
---
There are *supervised*, *unsupervised* and *reinforcement* learning. For now we focus on supervised learning algorithms.

The following sections list common approaches. ^3e5928

##### Supervised
How do we #supervise an algorithm?  What does it mean to supervise?  Supervision here means that *we provide a set of examples*, and then the *algorithm will use this set to produce* **predictions**.

First, we need to describe the *features*/attributes of the data set, i.e. *data representation*.  In the case of CSV format, the *feature values* are represented in comma-separated values.

```ad-note
The concept of *noise* will be discussed more in the future.  In brief, noise could appear as inaccurate labeling of data, for example.

```

- Tree:
	- [[01. Decision Trees|Decision Trees]]
	- Random forest
	- Regression Tree, Decision Stump, ...
- Instance:
	- [[02. Geometric Learning and KNN|KNN]]
- Kernel:
	- SVM
- Network:
	- [[04. Perceptron Decision Bounds|Perceptron]]
	- Linear Regression
	- Multi-layer
- Probabilistic Model:
	- Naive Bayes
	- Logistic Regression
	- Bayes Net, Deep Belief Net, Bayesian Optimal, ...

##### Unsupervised
- Clustering:
	- K-Means
	- DBScan
	- Hierarchical, Gaussian Mixture Models, ...
- Dimensionality Reduction:
	- PCA
	- SVD, LDA, ...
- Anomaly Detection
- Association Rule

##### Reinforcement
- Temporal Difference
- Q Learning
- MDP
- SARSA
- Deep Reinforcement Learning


## 3. The Induction Framework
---
> [!info] What is inductive learning?
> Given (i) a loss function $\ell$ and (ii) a sample $D$ from some unknown distribution $\mathcal{D}$, you must compute a function $f$ that has low expected error $\epsilon$ over $\mathcal{D}$ with respect to $f$.

*First introduced in [[01. Decision Trees]]*.
```

```mermaid
graph TD
	1[train data] --> 2[learning algo.] --> 3[f]
	3 --predicted label--> 4[test example] --> 3
	3 -.- 5["<strong>NOTE:</strong><br><em>f(x) -> y</em>, where<br><em>x: examples</em><br><em>y: label</em>"]
```


Types of Inductive Learning:
1. **Regression**: falls back toward the *mean* value
2. **Binary classification**: predicts a simple binary outcome
3. **Multiclass classification**: goes beyond binary to higher dimensional outcomes
4. Discovery
5. Reinforced learning

*We focus on 1-4 for now.*

Example:
	$f(x) \to R^+$
	$f(x) \to {0, 1} {+, -}$
	$f(x) \to { NYC, SF, Pullman, CA }$
	

### Inductive Bias
*Inductive bias* is the preference for one distinctive dimension during a classifying problem over another. For example, given a group of bird images and another group of non-flying mammal images, there are two ways one can classify the two groups: either "bird vs. non-bird" or "fly vs. no-fly".

Thus, the error generated from *inductive bias* is due to the limitation of the algorithm. Its own constraints prevent a particular algorithm to cope with the data that it is handling.

ML models differ from one another by their own *inductive bias*. Also, remember that **not everything is learnable**.


## 4. Performance
---
### i. Prediction Accuracy
$$Accuracy  = \frac {\mbox{number of correct}}{\mbox{total}}$$


### ii. Loss Function
##### a. Loss Function
We use a loss function to measure the performance of the learning algorithm:
- against unseen test data (whether algorithm is *generalizable*)
- whether it is relevant to the problem being solved
- that it is sees a strong algorithmic relationship between the training data and test data

The loss function typically has two parameters $\ell(\cdot, \cdot)$. It tells us *how bad* is a system performing in predicting compared to the truth. For example, the following function $L$ is a measure of error: $L(y, \hat{y})$, where $y$  = true label and $\hat{y}$ = system prediction.

*A loss function must be decided for every learning goal*. The loss function is used in:
- Regression:
	- *squared loss:*
		$$\ell(y, \hat{y})=(y-\hat{y})^2$$
	- *absolute loss:*
		$$\ell(y, \hat{y})=|y-\hat{y}|$$
- Binary classification:
	- *zero/one loss:*
		$$
		\ell(y, \hat{y})= \cases{ 0 & if $y=\hat{y}$ \cr
								   1 & otherwise }
		$$
- Multiclass classsification: also *zero/one loss*
- Discovery
- Reinforcement learning


##### b. Probabilistic Model
Given a probability distribution $\mathcal{D}$, the training algorithm must try to induce a function $f$ that should minimize *expected loss* $\epsilon$ over examples drawn from $\mathcal{D}$.


### iii. Overfitting and Underfitting

> [!important] 
> The goal of any learning algorithm is not try to get 0% error. The goal is that it will do well on *future*, *unseen* data.

*Underfitting* is when you had the opportunity to learn something but didn't. In contrast, *overfitting* is when you pay too much attention to the idiosyncracies of the training data, and aren't able to generalize well. Another good analogy is a lazy student who doesn't learn much given the opportunities and a nerd who spends too much time on useless information. Both missed the point of learning.

### iv. Hyperparameters
These are parameters that control other parameters in the model. In other words, changing the hyperparameters can influence the shape of the outcome in large scale.


## 99. Other Key Ideas
---
- *training error*
- variance and bias
- sigmoid function
- probabilistic modeling
- na√Øve Bayes
