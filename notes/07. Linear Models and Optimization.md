#optimization #regularization #error-rate #surrogate-loss #gradient-descent #linear-regression #support-vector-machines #svm


### 1. Optimization framework
---
We formalize the problem of finding good separating hyperplane (good hyperparameters) as a optimization problem mathematically:

$$
\DeclareMathOperator*{\min}{min}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0]
$$

Interpreting the above expression:
- *We optimize over $\boldsymbol{w}$ and $b$*.
- *We try to minimize the* **objective function**, i.e. finding the #error-rate  (in this case the **0/1 loss**) of the linear classifier parameterized by $\boldsymbol{w}$ and $b$.
- The expression $\boldsymbol{1}[\dots]$ is the **indicator function**, which is $=1$ when $[\dots]$ is true, otherwise 0.

>[!warning]
> This problem is NP-hard. **0/1 loss** is NP-hard to even *approximately minimize*. This is why we need to introduce a **regularizer**.


*A regularizer over the parameters of the model* can help ensure that the algorithm does not overfit the data. We have the following **regularized objective**:

$$
\DeclareMathOperator*{\min}{min}
\begin{aligned}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0] + \lambda\cdot R(w,b) \cr
\cr
\mbox{ where } R(w,b) \mbox{ is our regularizer function.}
\end{aligned}
$$

Given a formal objective, we ask the following questions:
- how can we adjust the optimization problem so that there *are* efficient (not NP-hard) algorithms to solve it?
- any good regularizers $R(\boldsymbol{w}, b)$?
- any algorithms that can efficiently solve this regularized objective, assuming that we can adjust the optimization problem appropriately?

We answer these question with the [[07. Linear Models and Optimization#2. Convex surrogate loss function|convex surrogate loss function]].

### 2. Convex surrogate loss function
---
*Reference: page 89.*

For **0/1 loss**, a positive margin, namely $y(\boldsymbol{w\cdot x}+b) > 0$, yields a loss of zero. Otherwise (negative margin) yields a loss of one.

Keywords: #concave, #convex, #function-chord.

>[!important]
> convex functions are great because they are *easy to minimize*, thus leading to **convex surrogate loss function**.
> 
> Imagine dropping a ball in a convex, *eventually* it will get to the minimum.


Because **0/1 loss** is hard to optimize, we look for something else to optimize instead. Thus, *we use a convex function to approximate 0/1 loss*. This method is called #surrogate-loss. The surrogate losses we construct will always be upper bounds on the true loss function. This guarantees that by minizing the surrogate loss we naturally also minimizing the real loss.

Given the reference **0/1 loss** defined in terms of the true label $y\{-1, +1\}$ and the predicted value $\langle\hat{y} = \boldsymbol{w\cdot x} + b\rangle$ as follows:
$$
	\ell^{\boldsymbol{0/1}}(y, \hat{y}) = \boldsymbol{1}[y\hat{y} \leq 0]
$$

we have four common surrogate loss functions: **hinge loss**, **logistic loss**, **exponential loss** and **squared loss**.

- hinge: **linear growth** for $\quad\hat{y} < 0$.
$$
\DeclareMathOperator*{\max}{max}
\ell^{hinge}(y, \hat{y}) = \max\{0, 1-y\hat{y}\}
$$
- logit: **linear growth** for $\quad\hat{y} < 0$.
$$
\ell^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\exp[-y\hat{y}])
$$
- exponential: **super-linear growth** for $\quad\hat{y}<0$.
$$
\ell^{exp}(y, \hat{y}) = \exp[-y\hat{y}]
$$
- square: **super-linear growth** for $\quad\hat{y}<0$.
$$
\ell^{exp}(y, \hat{y}) = (y-\hat{y})^2
$$

>[!note] how each deals with very confident correct predictions, i.e. $y\hat{y}>1$.
> - **hinge loss** does not care.
> - **logit** and **exp** can continue to optimize further.
> - **squared** it's equally bad to predict either $+3$ or $-1$ on a positive example.


### 3. Weight regularization
---
We answer the question *what should $R(\boldsymbol{w}, b)$ look like?*  in this section.

As mentioned before, R *has to be convex*. Next, we want the *components of the weight vector* (the scalars $w_d$) to be *small*. In other words, **we regularize the weight vector**. This is also our *inductive bias*.


### 4. Gradient Descent
---
*Reference: page 93.*

> [!important]
> **Gradient** is simply *derivative generalized for multidimensional space*.


> [!note] gradient ascent
> Suppose we try to find the maximum of a function $f(x)$. Our optimizer maintains a current estimate of the parameter $\boldsymbol{x}$.
> 
> At each step, it measures the *gradient* of the function at the current location $\boldsymbol{x}$, namely the gradient $\boldsymbol{g}$. It then take *in the direction* of the gradient, where the stepsize is controlled by a parameter $\eta$:
> $$ \boldsymbol{x} \leftarrow \boldsymbol{x} + \eta\cdot \boldsymbol{g} $$


#gradient-descent is the opposite of the basic gradient (gradient ascent), and our learning problems shall be framed as *minimization problems*. One of the major conditions for gradient descent is being able to find the true **global minimum**, i.e. *its objective function is convex*.

#### gradient descent algorithm
---
##### GradientDescent($\mathcal{F}$, $K$, $\eta_1\dots$)
---
$\boldsymbol{z_{0}} \leftarrow \langle 0, 0, \dots, 0\rangle$                                  // initialize variable we are optimizing
**for** $i = 1$ **to** $K$  **do**:
	$\boldsymbol{g_{i}} \leftarrow \nabla\mathcal{F}|_{z_{i-1}}$
	$\boldsymbol{z_{i}} \leftarrow \boldsymbol{z_{i-1}} - \eta_{i}\cdot\boldsymbol{g}_i$                           // compute the gradient at current location
**return** $z_K$

---


1. Gradients to Subgradients


### 5. Linear Regression
---
*Reference: page 97.*

- formally **closed-form optimization** for *squared loss*


### 6. Support Vector Machines (SVM)
---
*Reference: page 100.*


