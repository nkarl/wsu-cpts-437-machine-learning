#optimization #regularization #error-rate #surrogate-loss #gradient-descent #linear-regression #support-vector-machines #svm


## 1. Optimization framework
---
We formalize the problem of finding good separating hyperplane (good hyperparameters) as a optimization problem mathematically:

$$
\DeclareMathOperator*{\min}{min}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0]
$$

Interpreting the above expression:
- *We optimize over $\boldsymbol{w}$ and $b$*.
- *We try to minimize the* **objective function**, i.e. finding the #error-rate  (in this case the **0/1 loss**) of the linear classifier parameterized by $\boldsymbol{w}$ and $b$.
- The expression $\boldsymbol{1}[\dots]$ is the **indicator function**, which is $=1$ when $[\dots]$ is true, otherwise 0.

>[!warning]
> This problem is NP-hard. **0/1 loss** is NP-hard to even *approximately minimize*. This is why we need to introduce a **regularizer**.


**A regularizer** *over the parameters of the model* can help **ensure that the algorithm does not overfit the data**. We have the following *regularized objective*:
$$
\DeclareMathOperator*{\min}{min}
\begin{aligned}
&\min_{w,b} \sum_n \quad\underbrace{\boldsymbol{1}}_{\mbox{0/1 loss}}
\cdot[y_n(\boldsymbol{w\cdot x_n} +b) > 0] + 
\lambda\cdot \underbrace{R(w,b)}_{\mbox{regularizer}}
\cr
\cr
&\mbox{ where:}\cr
&\quad R(w,b) \mbox{: regularizer function }; \quad\lambda \mbox{: hyperparameter}

\end{aligned}
$$
^11900d

Given a formal objective, we ask the following questions:
- how can we adjust the optimization problem so that there *are* efficient (not NP-hard) algorithms to solve it?
- any good regularizers $R(\boldsymbol{w}, b)$?
- any algorithms that can efficiently solve this regularized objective, assuming that we can adjust the optimization problem appropriately?

We answer these question with the [[07. Linear Models and Optimization#2. Convex surrogate loss function|convex surrogate loss function]].


## 2. Convex surrogate loss function
---
*Reference: page 89.*

For **0/1 loss**, a positive margin, namely $y(\boldsymbol{w\cdot x}+b) > 0$, yields a loss of zero. Otherwise (negative margin) yields a loss of one.

Keywords: #concave, #convex, #function-chord.

>[!important]
> convex functions are great because they are *easy to minimize*, thus leading to **convex surrogate loss function**.
> 
> Imagine dropping a ball in a convex, *eventually* it will get to the minimum.


Because **0/1 loss** is hard to optimize, we look for something else to optimize instead. Thus, *we use a convex function to approximate 0/1 loss*. This method is called #surrogate-loss. The surrogate losses we construct will always be upper bounds on the true loss function. This guarantees that by minizing the surrogate loss we naturally also minimizing the real loss.

Given the reference **0/1 loss** defined in terms of the true label $y\{-1, +1\}$ and the predicted value $\langle\hat{y} = \boldsymbol{w\cdot x} + b\rangle$ as follows:
$$
	\ell^{\boldsymbol{0/1}}(y, \hat{y}) = \boldsymbol{1}[y\hat{y} \leq 0]
$$

we have four common surrogate loss functions: **hinge loss**, **logistic loss**, **exponential loss** and **squared loss**. ^5b8bb3

- hinge: **linear growth** for $\quad\hat{y} < 0$.
$$
\DeclareMathOperator*{\max}{max}
\ell^{hinge}(y, \hat{y}) = \max\{0, 1-y\hat{y}\}
$$
- logit: **linear growth** for $\quad\hat{y} < 0$. 
$$
\ell^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\exp[-y\hat{y}])
$$
^404f78
- exponential: **super-linear growth** for $\quad\hat{y}<0$.
$$
\ell^{exp}(y, \hat{y}) = \exp[-y\hat{y}]
$$
- square: **super-linear growth** for $\quad\hat{y}<0$.
$$
\ell^{exp}(y, \hat{y}) = (y-\hat{y})^2
$$

>[!note] how each deals with very confident correct predictions, i.e. $y\hat{y}>1$.
> - **hinge loss** does not care.
> - **logit** and **exp** can continue to optimize further.
> - **squared** it's equally bad to predict either $+3$ or $-1$ on a positive example.


## 3. Weight Regularization
---
We answer the question *what should $R(\boldsymbol{w}, b)$ look like?*  in this section.

We want two conditions:
- **R has to be [[07. Linear Models and Optimization#|convex]].
- **the components (scalars $w_d$) of the weight vector to be *small***. *In other words, we regularize the weight vector*. This is also our *inductive bias*.

From [[07. Linear Models and Optimization|the Regularized Ojective for 0/1 Loss]], we can **replace** the 0/1 loss function with **some surrogate loss** $\boldsymbol{\ell}$:

$$
\DeclareMathOperator*{\min}{min}
\begin{aligned}
\min_{w,b} \sum_n \quad\underbrace{\boldsymbol{\ell}}_{\mbox{surrogate loss}}\cdot[y_n(\boldsymbol{w\cdot x_n} +b) > 0] + \lambda\cdot R(w,b) \cr
\cr
\mbox{ where } R(w,b) \mbox{ is our regularizer function.}
\end{aligned}
$$

#### p-norms
In addition to small weights being good, you could argue that zero weights are better. If a weight wd goes to zero, then this means that feature d is not used at all in the classification decision. If there are a large number of irrelevant features, you might want as many weights
to go to zero as possible. This suggests an alternative regularizer:

$$
R^{(cnt)} (\boldsymbol{w}, b)= \sum_d\quad\boldsymbol{1}\cdot[x_d \neq 0]
$$

This line of thinking leads to the concept of $p$-**norms**.

$$
||w||_p = \left(\sum_d\quad|w_d|^p\right)^{1/p}
$$

> [!info]
> - Smaller values of $p$ prefer sparser vectors.
> - When $p < 1$ the norms become non-convex.


### 4. Gradient Descent
---
*Reference: page 93.*

> [!important]
> **Gradient** is simply **derivative *generalized for multidimensional space***.


> [!note] gradient ascent
> Suppose we try to find the maximum of a function $f(x)$. Our optimizer maintains a current estimate of the parameter $\boldsymbol{x}$.
> 
> At each step, it measures the *gradient* of the function at the current location $\boldsymbol{x}$, namely the gradient $\boldsymbol{g}$. It then take *in the direction* of the gradient, where the stepsize is controlled by a parameter $\eta$:
> $$ \boldsymbol{x} \leftarrow \boldsymbol{x} + \eta\cdot \boldsymbol{g} $$


#gradient-descent is the opposite:

$$
\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta\cdot \boldsymbol{g}
$$

Our learning problems shall be framed as *minimization problems*. One of the major conditions for gradient descent is being able to find the true **global minimum**, i.e. *its objective function is convex*.

#### gradient descent algorithm
---
##### GradientDescent($\mathcal{F}$, $K$, $\eta_1\dots$)
---
$\boldsymbol{z_{0}} \leftarrow \langle 0, 0, \dots, 0\rangle$                                  // initialize variable we are optimizing
**for** $i = 1$ **to** $K$  **do**:
	$\boldsymbol{g_{i}} \leftarrow \nabla\mathcal{F}|_{z_{i-1}}$
	$\boldsymbol{z_{i}} \leftarrow \boldsymbol{z_{i-1}} - \eta_{i}\cdot\boldsymbol{g}_i$                           // compute the gradient at current location
**return** $z_K$
<hr>


#### gradients to subgradients (hinge regularized)


### 5. Linear Regression
---
*Reference: page 97.*

- formally **closed-form optimization** for *squared loss*


### 6. Support Vector Machines (SVM)
---
*Reference: page 100.*
