#bayes #naive-bayes #logistic-loss #logistic-regression #regularization 


## 1. Classification by *density estimation*
---
### Probability Rules
- **random variables**
- **sum-to-one**
- **marginalization**
- **chain rule**
- **Bayes rule**


## 2. Statistic estimation
---
- log probability
- Lagrange multiplier
	- Lagrange variable
	- Lagrangian

#### Naive Bayes models
- chain rule
- naive Bayes assumption 
- Bernouilli distribution
- discrete distribution
- Gaussian/Normal distribution (continuous)

## 3. Prediction
---


## 4. Generative Stories
---
5. Generative stories


## 6. Conditional Models & Logistic Regression
---
	- #logistic-regression 

Given a logistic function:
$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
$$

**log-likelihood** is the *negative and scaled form* of the [[07. Linear Models and Optimization#^404f78|logistic loss for linear models]].


### 7. [[07. Linear Models and Optimization#3. Weight regularization|Regularization]] via Priors
---
*Reference: page 126.*

