#cnn #convolutional-neural-network #convolutional-neural-network 

## 0. TODO
---
- [ ] Make a copy of the Colab notebook provided by Dr. Cook
	- [ ] Annotate:
		- [ ] How to import image data
		- [ ] Choose features
		- [ ] Set up essential statistics (mean, variance)
- [ ] Design the model
	- [ ] Set up our own Colab notebook
	- [ ] Set up dataspace using Google Cloud or AWS (will look into which one later)
- [ ] Modeling
	- [ ] Build
		- [ ] Decide on train/test partitions
	- [ ] Test

## I. Problem
---
We want to classify some dataset of neurological images into some binary conclusion, for example either 'healthy' or 'diseased'.


## II. Data
---
Data obtained from [[Link later]].

Because Convolutional Networks are best for processing and classifying image data, we choose data that contain only images.

There are two possibilities for our data:

- Using fMRI data
- Using other data when no fMRI data is easily accessible (having to request access from site admin)

## III. Solution
---
We use a [[11. Convolutional Neural Networks|convolutional neural network]] to perform binary classification on the data set. Thus our loss function is a **0/1 loss**.

For reference, the *objective* is to try to minimize this function:
$$
\DeclareMathOperator*{\min}{min}
\min_{w, b}\sum_n \boldsymbol{\underbrace{L}_{\mbox{0/1 loss}}}\cdot\underbrace{\left[\boldsymbol{f_{CNN}}>0\right]}_{\mbox{activation}} + \underbrace{\lambda\cdot R(w, b)}_{\mbox{hyperparameter }\boldsymbol{\cdot}\mbox{ regularizer}}
$$


#### 1. Loss Function

Since ours is a binary classification problem, we will use *logistic regression*. We will surrogate the **0/1 loss** function with a logistic loss function:
$$
\boldsymbol{L}^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\exp[-y\hat{y}])
$$

> [!info]
> For reference, we have four common surrogate loss functions: **hinge loss**, **logistic loss**, **exponential loss** and **squared loss**. ^5b8bb3
> 
> - hinge: **linear growth** for $\quad\hat{y} < 0$.
> 	- Used in *SVM*
> $$
> \DeclareMathOperator*{\max}{max}
> \ell^{hinge}(y, \hat{y}) = \max\{0, 1-y\hat{y}\}
> $$
> - logistic/log: **linear growth** for $\quad\hat{y} < 0$. 
> $$
> \ell^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\exp[-y\hat{y}])
> $$
> ^404f78
> - exponential: **super-linear growth** for $\quad\hat{y}<0$.
> $$
> \ell^{exp}(y, \hat{y}) = \exp[-y\hat{y}]
> $$
> - square: **super-linear growth** for $\quad\hat{y}<0$.
> 	- Used in *linear regression*
> $$
> \ell^{exp}(y, \hat{y}) = (y-\hat{y})^2
> $$
> 


#### 2. Activation/Link function

Because our project use a neural network we use a sigmoid function, the **hyperbolic tangent**, as our activation/link function:

$$
\begin{align}
\hat{y}&=\sum v_i\cdot\tanh(w_i, \boldsymbol{\hat{x}})\cr
&=\boldsymbol{v}\cdot \tanh(\boldsymbol{W\cdot\hat{x}})
\end{align}
$$


#### 3. Weight regularization

Because ours is a neural network, we use back-propagation to regularize the weight vector.

