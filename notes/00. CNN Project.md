#cnn #convolutional-neural-network #convolutional-neural-network 

## 0. TODO
---
- [ ] Make a copy of the Colab notebook provided by Dr. Cook
	- [ ] Annotate:
		- [ ] How to import image data
		- [ ] Choose features
		- [ ] Set up essential statistics (mean, variance)
		- [ ] MNIST, [Microsoft Paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2003/08/icdar03.pdf)
- [ ] Design the model
	- [ ] Set up our own Colab notebook
	- [ ] Set up dataspace using Google Cloud or AWS (will look into which one later)
	- [ ] Modeling
		- [ ] Choose a dataset
			- [ ] Decide whether we should and how we will normalize the data
		- [ ] Build
			- [ ] Decide on train/test partitions
	- [ ] Test


## I. Problem
---
We want to classify some dataset of neurological images into some binary conclusion, for example:
- either *healthy* or *diseased*
- either *before* or *after*


## II. Data
---
Data obtained from [[Link later]].

Because Convolutional Networks are best for processing and classifying image data, we choose data that contain only images.

There are two possibilities for our data:
- Using fMRI data
- Using other data when no fMRI data is *not* easily accessible (having to request access from site admin)
	- document analysis

##### Normalization
- by feature
	- feature centering
	- feature scaling
- by example: easier to compare across the data sets.
$$
	x_n \leftarrow \frac{x_n}{||x_n||}
$$

##### Validation
- random sampling
- cross-validation (cycling through equisized partitions of the dataset)


## III. Solution
---
We use a [[11. Convolutional Neural Networks|convolutional neural network]] to perform binary classification on the data set. Thus our loss function is a **0/1 loss**.

For reference, the *objective* is to try to minimize this function:
$$
\DeclareMathOperator*{\min}{min}
\min_{w, b}\sum_n \boldsymbol{\underbrace{L}_{\mbox{0/1 loss}}}\cdot\underbrace{\boldsymbol{f_{CNN}}}_{\mbox{activation}} + \underbrace{\lambda\cdot R(w, b)}_{\mbox{hyperparameter }\boldsymbol{\cdot}\mbox{ regularizer}}
$$


#### 1. Loss Function
Since ours is a binary classification problem, we will use *logistic regression*. We will surrogate the **0/1 loss** function with a logistic loss function:
$$
\boldsymbol{L}^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\exp[-y\hat{y}])
$$

The following is the function form provided by [Google Machine Learning](https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training):

$$
\begin{align}
&\boldsymbol{L}^{logit}(y,\hat{y}) = \sum_{(x,y)\in D} -y\log(\hat{y}) - (1 - y)\log(1 - \hat{y})\cr
&\quad\text{where:}\cr
&\quad\quad\text{- $(x,y)\in D$: the labeled examples as $(x,y)$ pairs in the dataset.}\cr
&\quad\quad\text{- $y$: a labeled example. Must either be 0 or 1.}\cr
&\quad\quad\text{- $\hat{y}$: the predicted value where $0 < \hat{y} < 1$, given the set of features in $x$.}
\end{align}
$$

> [!info] Review
> For reference, we have four common surrogate loss functions: **hinge loss**, **logistic loss**, **exponential loss** and **squared loss**. ^5b8bb3
> 
> - hinge: **linear growth** for $\quad\hat{y} < 0$.
> 	- Used in *SVM*
> $$
> \DeclareMathOperator*{\max}{max}
> \ell^{hinge}(y, \hat{y}) = \max\{0, 1-y\hat{y}\}
> $$
> - logistic/log: **linear growth** for $\quad\hat{y} < 0$. 
> $$
> \ell^{logit}(y, \hat{y}) = \frac{1}{log2}log(1+\frac{1}{e^{y\hat{y}}})
> $$
> ^404f78
> - exponential: **super-linear growth** for $\quad\hat{y}<0$.
> $$
> \ell^{exp}(y, \hat{y}) = \exp[-y\hat{y}]
> $$
> - square: **super-linear growth** for $\quad\hat{y}<0$.
> 	- Used in *linear regression*
> $$
> \ell^{exp}(y, \hat{y}) = (y-\hat{y})^2
> $$
> 


#### 2. Activation/Link function
We have four options for our activation function:

- ReLU

- sigmoid

- softmax

- tanh
$$
\begin{align}
\hat{y}&=\sum v_i\cdot\tanh(w_i, \boldsymbol{\hat{x}})\cr
&=\boldsymbol{v}\cdot \tanh(\boldsymbol{W\cdot\hat{x}})
\end{align}
$$

#### 3. Weight regularization
Because ours is a neural network, we use back-propagation and gradient descent to regularize the weight vector.
