#gradient-descent #optimization #error-rate #surrogate-loss #regularization #convex-surrogate-loss-function


### 1. Optimization framework
---
We formalize the problem of finding good separating hyperplane (good hyperparameters) as a optimization problem mathematically:

$$
\DeclareMathOperator*{\min}{min}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0]
$$

Interpreting the above expression:
- *We optimize over $\boldsymbol{w}$ and $b$*.
- *We try to minimize the* **objective function**, in this case the **error rate** (**0/1 loss**) of the linear classifier parameterized by $\boldsymbol{w}$ and $b$.
- The expression $\boldsymbol{1}[\dots]$ is the **indicator function**, which is $=1$ when $[\dots]$ is true, otherwise 0.

>[!warning]
> This problem is NP-hard. **0/1 loss** is NP-hard to even *approximately minimize*. This is why we need to introduce a **regularizer**.

*A regularizer over the parameters of the model* can help ensure that the algorithm does not overfit the data. We have the following **regularized objective**:

$$
\DeclareMathOperator*{\min}{min}
\begin{aligned}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0] + \lambda\cdot R(w,b) \cr
\cr
\mbox{ where } R(w,b) \mbox{ is our regularizer function.}
\end{aligned}
$$

Given a formal objective, we ask the following questions:
- how can we adjust the optimization problem so that there *are* efficient (not NP-hard) algorithms to solve it?
- any good regularizers $R(\boldsymbol{w}, b)$?
- any algorithms that can efficiently solve this regularized objective, assuming that we can adjust the optimization problem appropriately?

We answer these question with the [[07. Linear Models#2. Convex surrogate loss function|convex surrogate loss function]].

### 2. Convex surrogate loss function
---
For **0/1 loss**, a positive margin, namely $y(\boldsymbol{w\cdot x}+b) > 0$, yields a loss of zero. Otherwise (negative margin) yields a loss of one.

Keywords: #concave, #convex, #function-chord.

>[!important]
> convex functions are great because they are *easy to minimize*, thus leading to **convex surrogate loss function**.
> 
> Imagine dropping a ball in a convex, *eventually* it will get to the minimum.

Because **0/1 loss** is hard to optimize, we look for something else to optimize instead. Thus, *we use a convex function to approximate 0/1 loss*. This method is called *surrogate loss*. The surrogate losses we construct will always be upper bounds on the true loss function. This guarantees that by minizing the surrogate loss we naturally also minimizing the real loss.

Four comon surrogate loss functions: **hinge loss**, **logistic loss**, **exponential loss** and **squared loss**.

- hinge:
$$
\mathcal{l}^{hinge} = 
$$

### 3. Weight regularization
---


### 4. Gradient Descent
---


5. Gradients to Subgradients