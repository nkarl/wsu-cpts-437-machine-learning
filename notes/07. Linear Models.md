#gradient-descent #optimization #error-rate #surrogate-loss #regularization #convex-surrogate-loss-function


### 1. Optimization framework
---
We formalize the problem of finding good separating hyperplane (good hyperparameters) as a optimization problem mathematically:

$$
\DeclareMathOperator*{\min}{min}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0]
$$

Interpreting the above expression:
- *We optimize over $\boldsymbol{w}$ and $b$*.
- *We try to minimize the* **objective function**, in this case the **error rate** (**0/1 loss**) of the linear classifier parameterized by $\boldsymbol{w}$ and $b$.
- The expression $\boldsymbol{1}[\dots]$ is the **indicator function**, which is $=1$ when $[\dots]$ is true, otherwise 0.

>[!warning]
> This problem is NP-hard. **0/1 loss** is NP-hard to even *approximately minimize*. This is why we need to introduce a **regularizer**.

*A regularizer over the parameters of the model* can help ensure that the algorithm does not overfit the data. We have the following **regularized objective**:

$$
\DeclareMathOperator*{\min}{min}
\begin{aligned}
\min_{w,b} \sum_n \boldsymbol{1}[y_n(\boldsymbol{w\cdot x_n} +b) > 0] + \lambda\cdot R(w,b) \cr
\cr
\mbox{ where } R(w,b) \mbox{ is our regularizer function.}
\end{aligned}
$$

Given a formal objective, we ask the following questions:
- how can we adjust the optimization problem so that there *are* efficient (not NP-hard) algorithms to solve it?
- any good regularizers $R(\boldsymbol{w}, b)$?
- any algorithms that can efficiently solve this regularized objective, assuming that we can adjust the optimization problem appropriately?

We answer these question with the [[07. Linear Models#2. Convex surrogate loss function|convex surrogate loss function]].

### 2. Convex surrogate loss function
---



### 3. Weight regularization
---


### 4. Gradient Descent
---


5. Gradients to Subgradients