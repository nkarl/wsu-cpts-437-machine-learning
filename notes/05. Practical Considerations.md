#normalization #feature-centering #feature-scaling #precision #recall #cross-validation #algorithm-robustness #bias-variance

Some categories of problems:
- pixel representation
- patch representation
- shape representation
- text categorization: bag of words

*If the input data is trash, the learning algorithm is unlikely to do well.*

Given a dataset, when the amount of bad features outnumber the good features *oftentimes to a large degree*, we need to check for the *robustness* of the learning algorithm.

1. #decision-tree : by limiting the tree's depth, we can certainly eliminate redundant features.
2. #KNN : irrelevant features can disrupt its performance, due to how in high dimensional space the distances of randomly distributed points become increasingly less important.
3. #perceptron : we can reasonably expect the algorithm to assign zero weight to irrelevant features.

### 1. Feature pruning & normalization
---
*Reference: page 58.*

It is often useful to #normalize data so that it is consistent. There are two basic types of #normalization:
- by feature
- by example

##### normalization by feature
*Each feature is adjusted in the same manner across all examples*.

There are two standards:
- #feature-centering: moving the entire set so that it's centered around the origin.
		$$
		x_{n,d} \leftarrow x_{n,d} - \mu_d \quad \mbox{where:} \quad \mu_d = \frac{1}{N} \sum_n x_{n,d}
		$$

- #feature-scaling: rescaling each feature so that one of the following holds:
		- #variance-scaling: each feature has *variance 1* across the training set
		$$
			- [ ] x_{n,d} \leftarrow \frac{x_{n,d}}{\sigma_d} \quad \mbox{where:} \quad \sigma_d = \sqrt{\frac{1}{N-1}\sum_n (x_{n,d}-\mu_d)^2}
		$$
		- #absolute-scaling: each feature has *maximum absolute value 1* across the training set
		$$
			\DeclareMathOperator*{\max}{max}
			x_{n,d} \leftarrow \frac{x_{n,d}}{r_d} \quad \mbox{where:} \quad  r_d = \max_n |x_{n,d}|
		$$


##### normalization by example
*Each feature is adjusted individually*. This is usually coupled with normalizing the length of each example vector, i.e. each example lies somewhere on the *unit hyperplane*.
$$
	x_n \leftarrow \frac{x_n}{||x_n||}
$$
This has the benefit of making the comparing task straightforward across data sets.


### 2. Combinatorial Feature Explosion
---
Perceptron has the most to gain from feature combination. Decision tree has the least to gain.


### 3. Performance evaluation & cross-validation
---
- #precision #recall
$$
\begin{aligned}
&P = \frac{I}{S} \cr
&R = \frac{I}{T}
\end{aligned}
$$

where,
	$S =$ number of Xs that the system found
	$T =$ number of Xs in the data
	$I =$ number of correct Xs that the system found

##### precision
How precise is the algorithm, i.e. how many output items X are actually X?

##### recall
How accurate is the algorithm, i.e. of all the Xs out there, how many were found?

##### f-score
The #f-score is the *harmonic mean* of precision and recall.
$$
F = \frac{2\times P\times R}{P + R}
$$

##### weighted f-score
The #weighted-f-score when precision is more important than recall.
$$
F_\beta = \frac{(1+\beta^2)\times P\times R}{\beta^2\times P + R} \quad \mbox{where} \quad \beta\in[0, \infty]
$$
for:
- $\beta = 1\quad$    then $\quad lim_{F_\beta} = F$
- $\beta = 0 \quad$    then  $\quad lim_{F_\beta} = 0$
- $\beta \to \infty \quad$ then  $\quad F_\beta \quad$  focuses entirely on the $P$


##### cross-validataion
- #cross-validation
- Break the training data into 10 equisized partitions, then proceed to train on 9 parts and then test on the remaining 1 part.
- Repeat this process for 10 times, each time holding out a different partition as the "development" part.
- Average out over the 10 cycles for an estimation of how well the algorithm will perform in the future.


### 4. Hypothesis testing and statistic signficance
---
*Reference: page 67*.

The goal of hypothesis testing is to compute #p-value.

- paired #t-test , is a form of parametric test, where $t$ is defined as:
	$$
	\begin{aligned}
	& t = (\mu_a - \mu_b)\sqrt{\frac{N(N-1)}{\sum_n(\hat{a}_n - \hat{b}_n)^2}} \cr
	& \cr
	& \quad \mbox{where:} \cr
	& \quad \quad \mu_a = \mbox{means of }a \cr
	& \quad \quad \mu_b = \mbox{means of }b
	\end{aligned}
	$$
- #bootstrapping


### 5. bias/variance trade-off
---
> [!important]
> $$\begin{aligned}\DeclareMathOperator*{\error}{error} \DeclareMathOperator*{\min}{min} &\error(f) = \left[\error(f) - \min_{f^*\in\mathcal{F}}\error(f^*)\right] + \left[\min_{f^*\in\mathcal{F}}\error(f)\right] \cr \cr &\quad \mbox{where:} \cr & \quad \quad \quad \mbox{1st term: estimation error}\cr & \quad \quad \quad \mbox{2nd term: approximation error}\end{aligned}$$


- #approximation-error
	- measures the quality of the *model family* (or *hypothesis class*)
	- #bias
	
- #estimation-error
	- measures the difference between the actual learned classifier $\boldsymbol{f}$ against the optimal classifier $\boldsymbol{f^*}$.
	- #variance
