## Geometric Learning & Nearest Neighbors

### Geometry & Dimensionality

Given a set of email data, we see if we can identify spam emails from regular emails. We aim to build feature vectors and let the algorithm to "learn". There are a few suggestions we can start from:
- subject length
- domain name of email
- punctuation
- grammar score
- capitalization
- words (targeted words)
- formatting
- No specific person being addressed

### K-Nearest Neighbors (KNN) Classifier

In a KNN algorithm, we give data points that are to one another the same label. In other words, instances should be similar to the points nearby.

$d(a,b) = [\sum_{d=1}^{D} (a_d - b_d)^2]^{1/2}$

### When can a distance function cause the classifer to fail?

1. Irrelevant features
2. Redudant features
3. Range of features

A way to counter this problem is to **normalize** data.

```python

def normalize(X):
	min_v = min(X)
	range = float(max(X) - min_v)
	new_list = [(x-min_v)/range for x in X]
	
```

### KNN Pros & Cons

#### Pros:
- simple
- no training
- good accuracy
- time series
- classfication or regression

#### Cons:
- expensive; does NOT scale well
- sensitive to features & distance
