#convolutional-neural-network 


## 1. Introduction
---
> [!abstract] 
> Convolutional neural networks are a specialized neural networks that use a mathematical operation called [[notes/math/Convolution|convolution]] in place of general matrix multiplication in at least one of their layers.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-13)
> 
> *They are specifically designed to process pixel data and are used in image recognition and processing*.

A CNN is a [[07. Linear Models and Optimization#3. Weight Regularization|regularized]] version of a graph of [[10. Neural Networks|multilayered perceptrons]]. Each neuron in on layer is fully connected to *all* neurons in the next layer. This is what it means to be *fully connected*, and this is the reason why these networks *are prone to overfitting*.

Typical ways of regularization:
- penalizing parameters during training
	- weight decay
- trimming connectivity
	- skipping connections, dropout, etc.

> [!info] A CNN uses a different approach.
> *It takes advantage of the hierarchical pattern in data*. It assembles (collects and aggregates) patterns of increasing complexity using smaller and simpler patterns embedded in its own filters.
> 
> *An anology to how CNN works is that it tries to build a tower from a bunch of lego bricks.*

##### Benefits of CNN
- smaler scale of connectivity and complexity compared to other algorithms.
- *little pre-processing needed* compared to other image-classification algorithms.


## 2. Architecture
---
A CNN has the following layers:
- an input layer
- hidden layers
- output layer

In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final [convolution](https://www.wikiwand.com/en/Convolution "Convolution").

*In a convolutional neural network*, the hidden layers include layers that perform convolutions. Typically this includes a layer that performs a [dot product](https://www.wikiwand.com/en/Dot_product "Dot product") of the convolution kernel with the layer's input matrix. This product is usually the [Frobenius inner product](https://www.wikiwand.com/en/Frobenius_inner_product "Frobenius inner product"), and its activation function is commonly [ReLU](https://www.wikiwand.com/en/Rectifier_(neural_networks) "Rectifier (neural networks)").

As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.

##### a. Convolutional Layers
In a CNN, the input is a [tensor](https://www.wikiwand.com/en/Tensor "Tensor") with a shape:

(number of inputs) $\times$ (input height) $\times$ (input width) $\times$ (input [channels](https://www.wikiwand.com/en/Channel_(digital_image) "Channel (digital image)"))

After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map [channels](https://www.wikiwand.com/en/Channel_(digital_image) "Channel (digital image)")).

> [!important]
> Convolutional layers convolve the input and pass its result to the next layer.
>
> This is similar to the response of a neuron in the visual cortex to a specific stimulus.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-deeplearning-14) Each convolutional neuron processes data only for its [receptive field](https://www.wikiwand.com/en/Receptive_field "Receptive field").
>
> > [!info]
> > Although [fully connected feedforward neural networks](https://www.wikiwand.com/en/Multilayer_perceptron "Multilayer perceptron") can be used to learn features and classify data, this architecture is *generally impractical for larger inputs* such as high-resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature.
> >
> > > [!example]
> > > For instance, a fully connected layer for a (small) image of size 100 × 100 has 10,000 weights for _each_ neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-15) For example, regardless of image size, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during [backpropagation](https://www.wikiwand.com/en/Backpropagation "Backpropagation") in traditional neural networks.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-16)[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-17) *Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images)*. This is because spatial relations between separate features are taken into account during convolution and/or pooling.

##### b. Pooling Layers
Convolutional networks may include *local and/or global pooling layers* along with traditional convolutional layers.

*Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer*. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-flexible-18)[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-19)

There are two common types of pooling in popular use:
- max
- average

_Max pooling_ uses the maximum value of each local cluster of neurons in the feature map,[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-Yamaguchi111990-20)[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-mcdns-21) while _average pooling_ takes the average value.

##### c. Fully-Connected Layers
Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional [multilayer perceptron](https://www.wikiwand.com/en/Multilayer_perceptron "Multilayer perceptron") neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.

##### d. Receptive Field
In neural networks, each neuron receives input from some number of locations in the previous layer.

In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's _receptive field_. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the _entire previous layer_.

Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes into account the value of a pixel, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.

##### e. Weights
Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). _**Learning** consists of iteratively adjusting these biases and weights_.

*The vectors of weights and biases* are called **filters** and represent particular [features](https://www.wikiwand.com/en/Feature_(machine_learning) "Feature (machine learning)") of the input (e.g., a particular shape).

A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the [memory footprint](https://www.wikiwand.com/en/Memory_footprint "Memory footprint") because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[](https://www.wikiwand.com/en/Convolutional_neural_network#cite_note-LeCun-22)
